\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{authblk}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{margin=1in}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{nameref}
\usepackage{placeins}
\usepackage{hyperref}
\usepackage{biblatex}
\usepackage[dvipsnames]{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={blue!80!black},
    citecolor={blue!80!black},
    urlcolor={blue!80!black}
}

\usepackage{lipsum}
\usepackage{enumitem}

% Command for writing comments
\newcommand\JC[1]{{\color{Maroon} JC: #1}}         % Jared:    Use \JC{my note}

\newcommand*{\doi}[1]{DOI: \href{http://dx.doi.org/#1}{#1}}\setitemize{noitemsep,topsep=5pt,parsep=2pt,partopsep=0pt}

% bibliography
\addbibresource{llms-and-language.bib}
% Redefine the bibliography heading to use \subsection size
\defbibheading{bibsection}[title]{%
  \subsection*{#1}%
}



% SETUP
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{problem}{Problem}

\title{Statement of Research Interest and Bibliography: LLMs and Endangered Language Revitalization}
\author{Jared Coleman}
% \affil{Loyola Marymount University}

\date{}

\begin{document}
\maketitle

% \begin{abstract}
% \lipsum[1]
% \end{abstract}


\section*{Introduction}
This document is meant to be an informal, ever-changing collection of interesting papers and resources related to Large Language Models (LLMs) and language revitalization.
LLMs have been shown to be remarkably capable at a wide variety of natural language tasks including machine translation, summarizing, question-and-answering, auto-completion, dialog, and more~\cite{gpt:agi}.
State-of-the-art LLMs are trained on vast amounts of natural language data from the internet~\cite{gpt:gpt4-tech-report} and, as a result, do not perform as well on tasks that involve low/no-resource languages~\cite{palm,gpt:low-resource-translation}.
We refer to languages with very little publicly available bilingual or monolingual corpora as ``low-resource'' languages and those with \textit{no} publicly available corpora as ``no-resource'' languages.

\subsection*{Research Questions}
In exploring how LLMs might be used for endangered language preservation and revitalization, we have identified the following research questions as some of the most interesting and important:
\begin{itemize}
    \item How do models ``know'' language? This is important for understanding how they might be taught new languages from scratch. By taught, I don't mean fine-tuned or trained (in the ML sense of the word ``train''). Rather, I mean \textit{taught} like a human is taught language: through dialog, question and answering, context, and experience.
    \begin{itemize}
        \item Black-box experimentation: The past few decades have seen many advances in linguistics through creative black-box experiments~\cite{atoms,through-the-language-glass,language-instinct}. Can these be recreated with LLMs? How might the results differ and what might that tell us about how LLMs ``know'' language?
        \item Linguistic Probing: We can perform experiments and ``brain-scan'' models to see which parts of the underlying network activate to better understand how they work! Interestingly, this has only relatively recently become possible (still with extreme limitations) for humans (via MRI).
        \item We care less about whether or not LLMs learn \textit{like} humans and more about understanding how LLMs learn so that we can leverage the knowledge to build useful tools for low/no-resource languages.
    \end{itemize}
    \item How can we use popular LLM tool-building techniques to create tools for the documentation, preservation, and revitalization of endangered languages?
    \begin{itemize}
        \item In the context window: few-shot learning, prompt engineering, function calling, etc. We proposed a new approach for low/no-resource language machine translation using a combination of these techniques~\cite{llm-rbmt}.
        \item Tokenization: Can adding tokens for target language words help with natural language tasks?
        \item Fine-tuning: with limited data, fine-tuning is difficult.
    \end{itemize}
    \item How can LLMs be used for foreign language education
    \begin{itemize}
        \item Ultimately, the goal of endangered language revitalization is to create new \textit{human} speakers.
        \item How can LLMs be used effectively in language education? We proposed a new approach for using LLMs as practice partners and tutors for language learning~\cite{yu2023chatlang}.
    \end{itemize}
\end{itemize}

\subsection*{Useful Tools Enabled by Research}
Pursuing the above research questions will guide and enable the development of many practically useful tools for endangered language revitalization.
Some of these include:
\begin{itemize}
    \item Parsing linguistic literature for grammar, vocabulary, etc.
    \item Summarizing/explaining content for language learners
    \item Grammar induction
    \item Auto-completion
    \item Data sanitization/standardization
    \item Adaptive data collection: using an LLM to help adjust the questions or queries made to native speakers during data collection to gather the most relevant and useful information.
\end{itemize}

\subsection*{Special Concerns for Indigenous Communities}
When working on language revitalization efforts with indigenous communities, history and context matter.
Genocide and forced assimilation~\cite{genocide} have led to the endangerment of many indigenous cultures and languages throughout the United States.
At Indian boarding schools, which were established to turn the surviving indigenous population into a servile class, children were forced to abandon their native languages and cultures~\cite{to-remain-an-indian}.

Even the more modern and well-intentioned efforts to document and revitalize indigenous languages are not without their own ethical concerns.
My tribe, for example, prohibits telling some traditional stories except during the winter.
To document these stories and make them publicly available throughout the year would undermine this culturally important tradition.
Different indigenous communities have different boundaries and rules for what is appropriate to share and what is not.
It is important to respect these boundaries and to work with communities to ensure that the work being done is culturally appropriate and respectful.

Finally, it is imperative that indigenous communities benefit from the work being done to document and revitalize their languages.
This means that the tools and resources developed should be made available to the communities in a way that is accessible and useful to them.
Another personal example: my great-grandmother was a fluent speaker of our language and so was the subject of a study by the University of California, San Diego Ph.D. student, Evan Norris.
His thesis "A Grammar Sketch And Comparative Study Of Eastern Mono"~\cite{mnr_grammar}, an invaluable resource for our critically endangered language, is locked behind a ProQuest academic paywall and is almost impossible for my family and other tribal members to access.

In our research on using LLMs for endangered language revitalization, we commit to respecting the boundaries and rules of the communities we work with and to making the research output accessible and useful to those communities.

\section*{Notes on Papers}
This section contains notes, summaries, and thoughts on some of the papers in the bibliography below.

\subsection*{Linguistic Probing}
\citetitle{probing:neural}~\cite{probing:neural} is a very nice introduction to linguistic probing.
The following excerpt, in particular, is very helpful in understanding the probing technique in general:
\begin{quote}
    As a simple example, we train an English-French NMT system on 110M tokens of bilingual data (English side). We then take 10K separate English sentences and label their voice as active or passive. We use the learned NMT encoder to convert these sentences into 10k corresponding 1000-dimension encoding vectors. We use 9000 sentences to train a logistic regression model to predict voice using the encoding cell states, and test on the other 1000 sentences. We achieve 92.8\% accuracy (Table 2), far above the majority class baseline (82.8\%). This means that in reducing the source sentence to a fixed-length vector, the NMT system has decided to store the voice of English sentences in an easily accessible way. When we carry out the same experiment on an English-English (auto-encoder) system, we find that English voice information is no longer easily accessed from the encoding vector. We can only predict it with 82.7\% accuracy, no better than chance. Thus, in learning to reproduce input English sentences, the seq2seq model decides to use the fixed-length encoding vector for other purposes.
\end{quote}

So, in general, the idea behind probing is to see whether a model is learning to encode certain known linguistic features as a byproduct of learning to perform a given task.
In particular, this example explores how the model is learning to encode the voice of the sentence (active or passive) as a linear combination of the encoding vectors.
Note that this approach is \textit{not} capable of telling us about linguistic features that may be encoded in a non-linear way, but it is a good start.

The paper \citeauthor{probing:bert}~\cite{probing:bert} is a very interesting follow-up to the previous paper that applies the probing technique to BERT and explores how different attention heads encode different linguistic features.
The paper \citeauthor{probing:parse-trees}~\cite{probing:parse-trees} uses probing techniques to perform experiments that suggest the BERT encoder is learning to encode parse tree distances in its hidden states.



\newpage
\section*{Bibliography}
The following bibliography is organized into different categories.
Some papers apply to more than one category and therefore appear multiple times.

\nocite{*}
\printbibliography[
    heading=bibsection,
    keyword={our-work},
    title={Our Work}
]
\printbibliography[
    heading=bibsection,
    keyword={llm},
    title={Work on LLMs (Large Language Models)}
]
\printbibliography[
    heading=bibsection,
    keyword={low-resource},
    title={Work on Low-Resource Languages}
]
\printbibliography[
    heading=bibsection,
    keyword={rbmt},
    title={Work on RBMT (Rule-Based Machine Translation)}
]
\printbibliography[
    heading=bibsection,
    keyword={rag},
    title={Work on RAG (Retrieval Augmented Generation)}
]
\printbibliography[
    heading=bibsection,
    keyword={embeddings},
    title={Work on Embeddings \& Semantic Similarity}
]
\printbibliography[
    heading=bibsection,
    keyword={linguistic-probing},
    title={Work on Linguistic Probing}
]

\printbibliography[
    heading=bibsection,
    keyword={embeddings-models},
    title={Embeddings Models}
]
\printbibliography[
    heading=bibsection,
    keyword={other},
    title={Other References}
]


\end{document}
